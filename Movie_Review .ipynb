{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de sentimiento mediante Recurrent Neural Networs usando el dataset publicado en http://ai.stanford.edu/~amaas/data/sentiment/ y procesado en un único .csv por https://www.kaggle.com/utathya/imdb-review-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review label         file\n",
       "0  test  Once again Mr. Costner has dragged out a movie...   neg      0_2.txt\n",
       "1  test  This is an example of why the majority of acti...   neg  10000_4.txt\n",
       "2  test  First of all I hate those moronic rappers, who...   neg  10001_1.txt\n",
       "3  test  Not even the Beatles could write songs everyon...   neg  10002_3.txt\n",
       "4  test  Brass pictures (movies is not a fitting word f...   neg  10003_3.txt"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"imdb_master.csv\", encoding='latin-1', index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled = data[data.label != 'unsup']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de reviews -->  50000\n",
      "Número total de reviews positivas -->  25000\n",
      "Número total de reviews negativas -->  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Número total de reviews --> \",len(data_labeled))\n",
    "print(\"Número total de reviews positivas --> \",len(data_labeled[data_labeled[\"label\"]=='pos']))\n",
    "print(\"Número total de reviews negativas --> \",len(data_labeled[data_labeled[\"label\"]=='neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_labeled['label'].apply(lambda x: 0 if x == 'neg' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la estructura de nuestra red neuronal y evitar un tiempo excesivo en la fase de entrenamiento, dividiremos el conjunto data_labeled en dos conjuntos, uno large y otro small. Reservaremos un subconjunto de 40000 reviews para entrenar mejor nuestra red neuronal más adelante. Y ahora trabajaremos con un conjunto de 10000 reviews, de las cuales 2500 serán nuestro conjunto de datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Aislamos nuestro conjunto de test y de train\n",
    "reviews_large, reviews_test, y_large, y_test = train_test_split(data_labeled['review'], y, test_size=2500, stratify=y)\n",
    "\n",
    "#Extraemos un subconjunto de entrenamiento de solo 7500 reviews\n",
    "reviews_rest, reviews_train, y_rest, y_train = train_test_split(reviews_large, y_large, test_size=7500, stratify=y_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfonso.arias\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dic = 1000 #Número máximo de palabras que tendrá nuestro diccionario.\n",
    "\n",
    "#El Tokenizer de Keras nos permite quedarnos con las palabras más frecuentes de todas las reviews\n",
    "diccionario = keras.preprocessing.text.Tokenizer(num_words = max_dic)\n",
    "diccionario.fit_on_texts(reviews_train)\n",
    "#Ahora, por cada review obtenemos un vector de enteros indicando la palabra del diccionario\n",
    "X_train = diccionario.texts_to_sequences(reviews_train)\n",
    "\n",
    "#Realizamos lo mismo para el data set de test\n",
    "X_test = diccionario.texts_to_sequences(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es recomendable que todas las reviews tengan la misma extensión de palabras\n",
    "max_palabras=300\n",
    "X_train=keras.preprocessing.sequence.pad_sequences(X_train,maxlen=max_palabras)\n",
    "X_test=keras.preprocessing.sequence.pad_sequences(X_test,maxlen=max_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 64)           64000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,449\n",
      "Trainable params: 76,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "red_neuronal=keras.models.Sequential()\n",
    "\n",
    "#Primera capa tipo embedding. Creamos un embedding de dimensión 64\n",
    "red_neuronal.add(keras.layers.embeddings.Embedding(input_dim=max_dic, input_length=max_palabras, output_dim=64))\n",
    "\n",
    "#Segunda capa tipo LSTM con 32 neuronas. Devuelve un vector después de procesar la secuencia completa\n",
    "red_neuronal.add(keras.layers.recurrent.LSTM(32))\n",
    "\n",
    "#Última capa que devuelve un valor entre 0 y 1\n",
    "red_neuronal.add(keras.layers.core.Dense(1))\n",
    "red_neuronal.add(keras.layers.core.Activation('sigmoid'))\n",
    "\n",
    "red_neuronal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_neuronal.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 40s 5ms/step - loss: 0.5490 - acc: 0.7089\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 33s 4ms/step - loss: 0.3827 - acc: 0.8351\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 29s 4ms/step - loss: 0.3363 - acc: 0.8636\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 37s 5ms/step - loss: 0.3039 - acc: 0.8777\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 31s 4ms/step - loss: 0.2782 - acc: 0.8897\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 29s 4ms/step - loss: 0.2754 - acc: 0.8903\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 29s 4ms/step - loss: 0.2621 - acc: 0.8955\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 30s 4ms/step - loss: 0.2344 - acc: 0.9104\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 29s 4ms/step - loss: 0.2315 - acc: 0.9104\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 30s 4ms/step - loss: 0.2092 - acc: 0.9180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272b5b60128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_neuronal.fit(X_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 2s 977us/step\n",
      "Test loss 0.41100694622993467\n",
      "Test accuracy 0.8168\n"
     ]
    }
   ],
   "source": [
    "validacion=red_neuronal.evaluate(X_test, y_test)\n",
    "print(\"Test loss\", validacion[0])\n",
    "print(\"Test accuracy\", validacion[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la precisión de nuestro conjunto de entrenamiento es mucho mayor que la de nuestro conjunto de test. Para solucionarlo, usaremos Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 64)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,449\n",
      "Trainable params: 76,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "red_neuronal=keras.models.Sequential()\n",
    "red_neuronal.add(keras.layers.embeddings.Embedding(input_dim=max_dic, input_length=max_palabras, output_dim=64))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.recurrent.LSTM(32,recurrent_dropout=0.45))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.core.Dense(1))\n",
    "red_neuronal.add(keras.layers.core.Activation('sigmoid'))\n",
    "red_neuronal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 37s 5ms/step - loss: 0.6530 - acc: 0.6219\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.5228 - acc: 0.7500\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4913 - acc: 0.7737\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4621 - acc: 0.7905\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4471 - acc: 0.7985\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4308 - acc: 0.8128\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4280 - acc: 0.8171\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4151 - acc: 0.8228\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 37s 5ms/step - loss: 0.4547 - acc: 0.7873\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.4021 - acc: 0.8264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272be31d470>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_neuronal.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "red_neuronal.fit(X_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 3s 1ms/step\n",
      "Test loss 0.44726420788764953\n",
      "Test accuracy 0.7952\n"
     ]
    }
   ],
   "source": [
    "validacion=red_neuronal.evaluate(X_test, y_test)\n",
    "print(\"Test loss\", validacion[0])\n",
    "print(\"Test accuracy\", validacion[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se regula mejor el overfitting. Vamos a añadir otro LSTM layer manteniendo los dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 300, 64)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 300, 32)           12416     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 84,769\n",
      "Trainable params: 84,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "red_neuronal=keras.models.Sequential()\n",
    "red_neuronal.add(keras.layers.embeddings.Embedding(input_dim=max_dic, input_length=max_palabras, output_dim=64))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.recurrent.LSTM(32,recurrent_dropout=0.45,return_sequences=True))\n",
    "red_neuronal.add(keras.layers.recurrent.LSTM(32))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.core.Dense(1))\n",
    "red_neuronal.add(keras.layers.core.Activation('sigmoid'))\n",
    "red_neuronal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 67s 9ms/step - loss: 0.5862 - acc: 0.6775\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.4439 - acc: 0.8040\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 58s 8ms/step - loss: 0.4036 - acc: 0.8257\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.3546 - acc: 0.8576\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.3363 - acc: 0.8608\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.3178 - acc: 0.8771\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.3227 - acc: 0.8689\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.2904 - acc: 0.8880\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.2802 - acc: 0.8883\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 59s 8ms/step - loss: 0.2861 - acc: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272c0647940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_neuronal.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "red_neuronal.fit(X_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 4s 2ms/step\n",
      "Test loss 0.4061969689130783\n",
      "Test accuracy 0.8132\n"
     ]
    }
   ],
   "source": [
    "validacion=red_neuronal.evaluate(X_test, y_test)\n",
    "print(\"Test loss\", validacion[0])\n",
    "print(\"Test accuracy\", validacion[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión en nuestro test data set no mejora añadiendo un layer. Vamos a pasar a probar a entrenar un modelo con las 40000 reviews que no hemos usado junto con nuestro training set actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debemos usar el dataset grande para definir un nuevo diccionario\n",
    "diccionario_large = keras.preprocessing.text.Tokenizer(num_words = max_dic)\n",
    "diccionario_large.fit_on_texts(reviews_train)\n",
    "X_train_large = diccionario_large.texts_to_sequences(reviews_large)\n",
    "X_test_large = diccionario_large.texts_to_sequences(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_palabras=300\n",
    "X_train_large=keras.preprocessing.sequence.pad_sequences(X_train_large,maxlen=max_palabras)\n",
    "X_test_large=keras.preprocessing.sequence.pad_sequences(X_test_large,maxlen=max_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 300, 64)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,449\n",
      "Trainable params: 76,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "red_neuronal=keras.models.Sequential()\n",
    "red_neuronal.add(keras.layers.embeddings.Embedding(input_dim=max_dic, input_length=max_palabras, output_dim=64))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.recurrent.LSTM(32,recurrent_dropout=0.45))\n",
    "red_neuronal.add(keras.layers.core.Dropout(0.45))\n",
    "red_neuronal.add(keras.layers.core.Dense(1))\n",
    "red_neuronal.add(keras.layers.core.Activation('sigmoid'))\n",
    "red_neuronal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47500/47500 [==============================] - 243s 5ms/step - loss: 0.5182 - acc: 0.7517\n",
      "Epoch 2/5\n",
      "47500/47500 [==============================] - 245s 5ms/step - loss: 0.4626 - acc: 0.7929\n",
      "Epoch 3/5\n",
      "47500/47500 [==============================] - 245s 5ms/step - loss: 0.4447 - acc: 0.8047\n",
      "Epoch 4/5\n",
      "47500/47500 [==============================] - 244s 5ms/step - loss: 0.4118 - acc: 0.8243\n",
      "Epoch 5/5\n",
      "47500/47500 [==============================] - 247s 5ms/step - loss: 0.4009 - acc: 0.8277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272c303dba8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_neuronal.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "red_neuronal.fit(X_train_large, y_large, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 3s 1ms/step\n",
      "Test loss 0.35239518189430236\n",
      "Test accuracy 0.8472\n"
     ]
    }
   ],
   "source": [
    "validacion=red_neuronal.evaluate(X_test, y_test)\n",
    "print(\"Test loss\", validacion[0])\n",
    "print(\"Test accuracy\", validacion[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
